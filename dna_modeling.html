<!DOCTYPE HTML>
<html>

<head>
    <title>DNA Modeling | Aymen Kallala</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" crossorigin="anonymous"
        onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '\\(', right: '\\)', display: false}]});">
    </script>
    <style>
        body {
            font-family: "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            background-color: #f5f5f5;
            color: #333;
            margin: 0;
            padding: 0;
        }

        #header {
            background-color: #282b30;
            color: #ffffff;
            padding: 20px 0;
            text-align: center;
            font-size: 2em;
            font-weight: bold;
        }

        #subheader {
            text-align: center;
            font-size: 1.2em;
            color: #333;
            margin-bottom: 20px;
        }

        #main {
            padding: 20px;
            max-width: 80%;
            margin: 0 auto;
            background-color: #ffffff;
            color: #333;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
        }

        h1 {
            color: #333;
            font-size: 1.75em;
            margin-bottom: 10px;
        }

        h2 {
            color: #333;
            font-size: 1.5em;
            margin-top: 1.5em;
        }

        h3 {
            color: #333;
            font-size: 1.25em;
            margin-top: 1em;
        }

        p,
        ul {
            margin: 1em 0;
        }

        ul {
            padding-left: 20px;
        }

        a {
            color: #333;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        blockquote {
            margin: 20px 0;
            padding: 10px 20px;
            background-color: #f0f0f0;
            border-left: 5px solid #333;
            font-style: italic;
            color: #666;
        }

        #footer {
            background-color: #f5f5f5;
            color: #666;
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
        }

        .icons a {
            color: #666;
            margin: 0 10px;
        }

        .icons a:hover {
            color: #333;
        }

        .toc {
            background-color: #e0e0e0;
            border: 1px solid #ccc;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 5px;
        }

        .toc summary {
            font-weight: bold;
            cursor: pointer;
            font-size: 1.2em;
            color: #333;
        }

        .toc ul {
            list-style: none;
            padding-left: 0;
            margin-top: 10px;
        }

        .toc li {
            margin-bottom: 5px;
        }

        .illustration-placeholder {
    width: 100%; /* Ensures the container takes up the full width of its parent */
    max-width: 100%; /* Ensures the container doesn't exceed the parent's width */
    overflow: hidden; /* Prevents the image from overflowing the container */
    margin: 20px 0; /* Adds some space around the container */
    text-align: center; /* Centers the image inside the container */
}
.content-container {
        width: 100%; /* Adjust the percentage to make it wider */
        margin: 0 auto; /* Center the content */
    }

    /* Code block styling */
    pre {
    background-color: #f8f8f8;
    border: 1px solid #ddd;
    padding: 10px;
    border-radius: 5px;
    font-size: 14px;
    overflow-x: auto; /* Allow horizontal scrolling */
    white-space: pre; /* Preserve formatting and allow horizontal scrolling without line breaks */
}

code {
    font-family: 'Courier New', Courier, monospace;
    font-size: 14px;
    color: #333;
}

.back-button {
        text-align: center;
        margin-top: 20px;
    }

    .back-button a {
        display: inline-block;
        padding: 10px 20px;
        background-color: #282b30;
        color: #ffffff;
        text-decoration: none;
        border-radius: 5px;
        box-shadow: 0px 2px 5px rgba(0, 0, 0, 0.2);
        transition: background-color 0.3s ease, box-shadow 0.3s ease;
        font-weight: bold;
    }

    .back-button a:hover {
        background-color: #3b4045;
        box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.3);
    }

    </style>
</head>

<body>

    <!-- Wrapper -->
    <div class="content-container">

        <!-- Header -->
        <header id="header">
            Exploring DNA Modeling with State Space Models
        </header>

        <div id="subheader">
            Aymen Kallala
        </div>

        <!-- Illustration -->
        <div style="text-align: center; margin-bottom: 20px;">
            <img src="images/dna_modeling/dna_strand.png" alt="Sampling Strategies Illustration" style="max-width: 50%; height: auto; border-radius: 5px;">
        </div>

        <!-- Main -->
        <div id="main">

            <!-- Table of Contents -->
            <details class="toc">
                <summary>Table of Contents</summary>
                <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#model-architecture">Model Architecture</a></li>
                    <li><a href="#dataset">Dataset</a></li>
                    <li><a href="#training">Pretraining</a></li>
                    <li><a href="#benchmarking">Benchmarking the models</a></li>
                    <li><a href="#how-to-use">How to Use</a></li>
                </ul>
            </details>

            <!-- Post -->
            <section class="post">
                <h1 id="introduction">Introduction</h1>
<p>This blog post explores the implementation of a Genomics Foundation Model using PyTorch Lightning. The aim is to create a clean, adaptable foundation for research, combining enough manual customization to enable easy modifications while leveraging tools for optimized training and monitoring.</p>
<p>State Space Models (SSMs) have recently generated considerable interest due to their efficiency and superior capability in managing long-range dependencies. Since the release of the <a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba architecture</a>, research groups across various domains have adopted it as a drop-in replacement for transformer blocks in many architectures. While language tasks remain the primary focus for performance comparisons, Mamba has also found applications in other fields.</p>
<p>DNA modeling, known for its complexity and demand for long-context understanding, has particularly benefited from these innovations. Recently, <a href="https://arxiv.org/abs/2403.03234" target="_blank">Caduceus</a>, a bi-directional Mamba-based encoder, was open-sourced, introducing a novel class of DNA Foundation Models that sets a new benchmark for genomic analysis. Let's train a Caduceus model and see how it performs :)</p>
<p><a href="https://github.com/AymenKallala/dna-foundation-model-workflow" target="_blank">Code repository</a></p>
<h2 id="model-architecture">Model Architecture</h2>

<p><a href="https://arxiv.org/abs/2403.03234" target="_blank">Caduceus Paper</a></p>
<p><a href="https://github.com/kuleshov-group/caduceus" target="_blank">Caduceus Repo</a></p>

                <p>Caduceus is a state-of-the-art architecture designed to address key challenges in genomic sequence modeling, specifically the need for long-range interaction modeling and reverse complementarity (RC) in DNA sequences. Built upon the Mamba block, Caduceus introduces two extensions: the BiMamba component for bi-directionality and the MambaDNA block for RC equivariance. These innovations enable Caduceus to model both forward and reverse sequences efficiently while maintaining parameter efficiency. This architecture supports the first family of RC-equivariant bi-directional DNA models, outperforming larger models that lack bi-directionality and equivariance on long-range genomic tasks .</p>
                <div class="illustration-placeholder">
                    <img src="images/dna_modeling/caduceus_architecture.png" alt="Caduceus" style="max-width: 100%; height: auto; border-radius: 5px;">
                </div>
                <p>For the purpose of this blog I will train Caduceus models of <strong>50M</strong>  parameters, and see that we can already match some great results on genomics standard benchmarks. One thing we can also do is to integrate Mamba2 into the initial caduceus model and check on potential improvements.</p>
                <p>My models are composed of:</p>
                <ul>
                    <li>A vocab size of 16</li>
                    <li>Embedding size of 768 (that gets multiplied by two because Caduceus adds RC-equivariance)</li>
                    <li>12 layers</li>
                    <li>a d_state of 64 for Mamba2 blocks (the dimension of the hidden layer)</li>
                </ul>

                <div class="illustration-placeholder">
                    <img src="images/dna_modeling/caduceus_architecture2.png" alt="Caduceus" style="max-width: 100%; height: auto; border-radius: 5px;">
                </div>

                <h2 id="dataset">Dataset</h2>
                <p>Before we dive into the training process, let’s take a moment to introduce the dataset we'll be using. With the rapid advancements in DNA modeling this year, DNA datasets have also grown significantly. Some of the most popular datasets for DNA modeling include HG38, 1000 Genomes (1000G), and multispecies datasets, which have been instrumental in training state-of-the-art models like the Nucleotide Transformer. For this blog, I’ll be using the HG38 dataset, widely recognized as the 'reference' human genome, consisting of approximately 3.2 billion nucleotides.</p>
                <p>Good thing, the HG38 dataset is open-source and easy to download using this command.</p>
                <pre><code>mkdir -p data/hg38/
                curl https://storage.googleapis.com/basenji_barnyard2/hg38.ml.fa.gz > data/hg38/hg38.ml.fa.gz
                gunzip data/hg38/hg38.ml.fa.gz  # unzip the fasta file
                curl https://storage.googleapis.com/basenji_barnyard2/sequences_human.bed > data/hg38/human-sequences.bed</code></pre>

                <p>There is ~24 chromosomes in the whole genome (merged into 1 fasta file), each chromosome is a continuous sequence, basically. Then the .bed file composes the sequences intervals (contains chromosome name, start, end, split, which then allow you to retrieve from the fasta file).
                The key component of the dataset implementation is the Fasta Interval class that allows you to wrap your dataset in the right format for our DNA model.</p>

                <details>
                    <summary>View FastaInterval code snippet</summary><pre><code>
                        class FastaInterval:
                    """Retrieves sequences from a fasta file given a chromosome and start/end indices."""
                    def __init__(
                            self,
                            *,
                            fasta_file,
                            return_seq_indices=False,
                            rc_aug=False,
                            local=False
                    ):
                        fasta_file = Path("./data/hg38/hg38.ml.fa")
                        assert fasta_file.exists(), "Path to fasta file must exist!"
                        self.seqs = Fasta(str(fasta_file))
                
                        self.return_seq_indices = return_seq_indices
                        self.rc_aug = rc_aug
                
                        # calc len of each chromosome in fasta file, store in dict
                        self.chr_lens = {}
                
                        for chr_name in self.seqs.keys():
                            self.chr_lens[chr_name] = len(self.seqs[chr_name])
                
                    @staticmethod
                    def _compute_interval(start, end, max_length, i_shift):
                        if max_length == MAX_ALLOWED_LENGTH:
                            return start, end
                        if max_length < MAX_ALLOWED_LENGTH:
                            #assert MAX_ALLOWED_LENGTH % max_length == 0
                            return start + i_shift * max_length, start + (i_shift + 1) * max_length
                        else:
                            raise ValueError(f"`max_length` {max_length} (> 2^{int(math.log(MAX_ALLOWED_LENGTH, 2))}) is too large!")
                
                    def __call__(
                            self,
                            chr_name,
                            start,
                            end,
                            max_length,
                            i_shift,
                            return_augs=False,
                    ):
                        """
                        max_length passed from dataset, not from init
                        """
                        chromosome = self.seqs[chr_name]
                        chromosome_length = self.chr_lens[chr_name]
                
                        start, end = self._compute_interval(start, end, max_length, i_shift)
                
                        if end > chromosome_length:
                            # Shift interval down
                            start = start - (end - chromosome_length)
                            end = chromosome_length
                            assert start == chromosome_length - max_length
                
                        if start < 0:
                            # Shift interval up
                            end = end - start
                            start = 0
                            assert end == max_length
                
                        if end > chromosome_length:
                            # This may occur if start + MAX_ALLOWED_LENGTH extends beyond the end of the chromosome
                            start = chromosome_length - max_length
                            end = chromosome_length
                
                        seq = str(chromosome[start:end])
                
                        if self.rc_aug and coin_flip():
                            seq = string_reverse_complement(seq)
                
                        return seq</code></pre></details>
                <p>Now, we can wrap all of this into a Dataset class and a Lightning Data Module to start trainings.</p>
                <details>
                    <summary>View Data Modules key snippet</summary><pre><code>
                        def __getitem__(self, idx):
        """Returns a sequence of specified len"""
        # sample a random row from df
        row_idx, shift_idx = idx // self.shifts, idx % self.shifts
        row = self.df.iloc[row_idx]
        chr_name, start, end = (row.iloc[0], row.iloc[1], row.iloc[2])

        seq = self.fasta(
            chr_name,
            start,
            end,
            max_length=self.max_length,
            i_shift=shift_idx,
            return_augs=self.return_augs,
        )
        if end - start != MAX_ALLOWED_LENGTH:
            print(row, "\nLength: ", end - start)

        if self.tokenizer_name == "char":
            seq = self.tokenizer(
                seq,
                padding="max_length",
                max_length=self.pad_max_length,
                truncation=True,
                add_special_tokens=False
            )

            seq = seq["input_ids"]  # get input_ids

            # need to handle eos here
            if self.add_eos:
                # append list seems to be faster than append tensor
                seq.append(self.tokenizer.sep_token_id)

        elif self.tokenizer_name == "bpe":
            seq = self.tokenizer(
                seq,
                # add_special_tokens=False,
                padding="max_length",
                max_length=self.pad_max_length,
                truncation=True,
            )
            # get input_ids
            if self.add_eos:
                seq = seq["input_ids"][1:]  # remove the bos, keep the eos token
            else:
                seq = seq["input_ids"][1:-1]  # remove both special tokens

        # convert to tensor
        seq = torch.LongTensor(seq)

        # replace N token with a pad token, so we can ignore it in the loss
        seq = self.replace_value(seq, self.tokenizer._vocab_str_to_int["N"], self.tokenizer.pad_token_id)

        if self.mlm:
            data, target = mlm_getitem(
                seq,
                mlm_probability=self.mlm_probability,
                contains_eos=self.add_eos,
                tokenizer=self.tokenizer,
                eligible_replacements=self.eligible_replacements,
            )

        else:
            data = seq[:-1].clone()
            target = seq[1:].clone()

        return data, target</code></pre></details>
                <h2 id="training">Pretraining</h2>

                <p>Training was done using a variety of optimization techniques that are very common in the Language Model pretraining landscape, and easy to implement with Lightning:</p>
                <ul>
                    <li><strong>Scheduled Learning Rate</strong>: Warmup for 10% of the training steps until reaching a 8e-4 peak and then cosine annealing until the end of training.
                        <div class="illustration-placeholder">
                            <img src="images/dna_modeling/cosine_schedule.png" alt="Caduceus" style="max-width: 100%; height: auto; border-radius: 5px;">
                        </div>
                    <li><strong>Mixed Precision Training</strong>: for computational efficiency and memory usage.</li>
                    <li><strong>Gradient Clipping</strong>: Set to 1.0 to avoid gradient exploding.</li>
                    <li><strong>Gradient Accumulation</strong>: To allow a large batch size with the hardware at hand. After experimentation, I found that a global batch size of 1.5M tokens was a great value that trades-off speed and training stability.</li>
                    <li><strong>Weight Decay</strong>: Set to 0.2.</li>
                    <li><strong>Gradient Checkpointing</strong>: To avoids redundant gradient recomputation and save some memory space.</li>
                    <li><strong>DDP (Distributed Data Parallel)</strong>: Facilitates parallel data processing on multiple GPUs, accelerating training times significantly.</li>
                    </ul>

                    
                    <h3 id="results">Results</h2>
                <p> Given that Caduceus is a bidirectionnal encoder model, we can train it following a BERT-style: with <strong> Masked Language Modeling</strong></p>
                <p>As a test for the small tweaks I added to the initial caduceus code, I trained two equally sized and parametrized models. One with Mamba1 blocks and the other with Mamba2 blocks. I trained my models on around 10B tokens each, and we can check the validation losses for both.</p>
        
                <div class="illustration-placeholder">
                    <img src="images/dna_modeling/loss_curve.png" alt="Caduceus" style="max-width: 120%; height: auto; border-radius: 5px;">
                </div>
                <p>It looks good! Both models seem to have converged to a plateau, and the Mamba2 version seems to be slightly better than the initial Mamba1 version!</p>
                <h2 id="benchmarking">Benchmarking the models</h2>
                <p>Incoming</p>
                <h2 id="how-to-use">How to Use This Implementation</h2>
                <p>You can find detailed instructions on the <a href="https://github.com/AymenKallala/dna-foundation-model-workflow">GitHub repository</a>. It includes setup, configuration, and usage instructions.</p>
            </section>
        </div>

        <!-- Back Button -->
        <div class="back-button">
            <a href="index.html">Return to Main Page</a>
        </div>

        <!-- Footer -->
        <footer id="footer">
            <p>&copy; 2024 Aymen Kallala</p>
            <ul class="icons">
                <li><a href="https://linkedin.com/in/aymenkallala" class="icon brands fa-linkedin"></a></li>
                <li><a href="https://github.com/AymenKallala" class="icon brands fa-github"></a></li>
            </ul>
        </footer>

    </div>

</body>

</html>
